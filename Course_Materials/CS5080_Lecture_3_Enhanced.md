# CS 5080 Lecture 3 - Markov Decision Processes (Enhanced)
**Date:** Tuesday, January 27, 2026, 4:45 PM
**Source:** Chapter 2 of Grokking Deep Reinforcement Learning (Morales)

**Legend:**
- Regular text = Your lecture notes (verified correct)
- âš ï¸ = Correction or clarification needed
- ğŸ“š **[ADDITIONAL FROM TEXTBOOK]** = Supplementary information from Chapter 2

---

## Markov Chains

### Weather Example
- **States:** S = {cold, rain, sun}
- **Transition Table T:**

| From/To | Cold | Rain | Sun |
|---------|------|------|-----|
| Cold | 0.2 | 0.5 | 0.3 |
| Rain | 0.5 | 0.3 | 0.2 |
| Sun | 0.5 | 0.1 | 0.4 |

*Note: Each row sums to 1* âœ“ Correct

ğŸ“š **[ADDITIONAL FROM TEXTBOOK]:** This table is called a **transition matrix** or **stochastic matrix**. Each entry T[i,j] represents P(next_state = j | current_state = i). The requirement that rows sum to 1 ensures it's a valid probability distribution.

### Markov Property (Simplifying Assumption)
- **The current state encapsulates all effects of history** âœ“ Correct
- Next transition depends *only* on the current state âœ“ Correct
- History of the chain: C, R, R, R, S, S, C, S, ...
  - "What will it be tomorrow?" depends on current state only âœ“ Correct

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Formal Definition]:**
> P(Sâ‚œâ‚Šâ‚ | Sâ‚œ, Aâ‚œ, Sâ‚œâ‚‹â‚, Aâ‚œâ‚‹â‚, ..., Sâ‚€, Aâ‚€) = P(Sâ‚œâ‚Šâ‚ | Sâ‚œ, Aâ‚œ)

The probability of the next state given the current state and action is **independent of all previous states and actions**. This is the **memoryless property**.

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Why This Matters]:**
Because RL agents are designed assuming Markov property, you must feed agents all necessary variables to make the property hold. For example:
- **Spacecraft landing:** Need position AND velocity (not just position)
- **Grid worlds:** Cell ID alone is sufficient
- The more variables you add â†’ longer training but more accurate
- Fewer variables â†’ faster but may not learn well

### Stock Market Example
States: Bear Market, Bull Market, Stagnant Market
- Bear â†’ Bull: 0.3
- Bear â†’ Bear: 0.5
- Bear â†’ Stagnant: 0.2
- (with self-loops and other transitions) âœ“ Correct

---

## Markov Decision Process (MDP)

### Definition (Finite MDP)
- **S** = set of states = {xâ‚, xâ‚‚, ..., xâ‚™} âœ“ Correct
- **A** = set of actions = {aâ‚, aâ‚‚, ..., aâ‚˜} âœ“ Correct
- **Agent** interacts with environment âœ“ Correct
- At time t: Sâ‚œ âˆˆ States, Aâ‚œ âˆˆ Actions âœ“ Correct

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Complete MDP Definition]:**
An MDP is formally defined by the tuple: **(S, A, T, R, Î³, H, Sáµ¢)**

| Component | Symbol | Description |
|-----------|--------|-------------|
| State Space | S | Set of all possible states |
| Action Space | A(s) | Set of actions available in state s |
| **Transition Function** | **T(s,a,s')** | **Probability P(s' \| s, a)** |
| **Reward Function** | **R(s,a,s')** | **Scalar reward for transition** |
| **Discount Factor** | **Î³** | **Value in [0,1], typically 0.99** |
| Horizon | H | Finite or infinite time steps |
| Initial State Distribution | Sáµ¢ | Where episodes start |

*Your notes covered S and A. The transition function T, reward function R, and discount factor Î³ are critical components covered later in the chapter.*

### Agent-Environment Interaction
- Agent can perform a set of actions A = {aâ‚, aâ‚‚, ..., aâ‚˜} âœ“ Correct
- Whenever an agent performs an action, it gets an **immediate reward Râ‚œâ‚Šâ‚** âœ“ Correct
- It transitions to state **Sâ‚œâ‚Šâ‚** âœ“ Correct

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - The RL Cycle]:**
```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                      â”‚
    â–¼                                      â”‚
  Agent â”€â”€Action Aâ‚œâ”€â”€â–º Environment         â”‚
    â–²                      â”‚               â”‚
    â”‚                      â”‚               â”‚
    â””â”€â”€â”€â”€ Sâ‚œâ‚Šâ‚, Râ‚œâ‚Šâ‚ â—„â”€â”€â”€â”€â”˜               â”‚
                                           â”‚
    (Repeat for each time step t) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Experience Tuple]:**
Each interaction produces an **experience tuple**: (Sâ‚œ, Aâ‚œ, Râ‚œâ‚Šâ‚, Sâ‚œâ‚Šâ‚)
- This is the fundamental unit of data for learning

### "Maze" as Markov Chain vs MDP
**Markov Chain:** States only, no agent control âœ“ Correct
**MDP:** Agent chooses actions to influence transitions âœ“ Correct

**Maze MDP:**
- S = {xâ‚, xâ‚‚, ..., xâ‚†} âœ“ Correct
- A = {â†‘, â†“, â†’, â†} âœ“ Correct
- The agent performs action Aâ‚œ in state Sâ‚œ to make transitions happen âœ“ Correct

ğŸ“š **[ADDITIONAL FROM TEXTBOOK]:**
The key difference: In a Markov Chain, transitions happen automatically according to probabilities. In an MDP, the **agent chooses** which action to take, and then transitions happen according to T(s,a,s').

### Rewards
- **Sparse rewards:** No immediate reward, only on reaching goal state âœ“ Correct
- **Example - Chess:** Taking pieces vs. winning âœ“ Correct
  - Rewards for taking pieces vs. only reward for winning
  - Different reward structures lead to different behaviors âœ“ Correct

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Dense vs Sparse Rewards]:**
| Reward Type | Description | Pros | Cons |
|-------------|-------------|------|------|
| **Dense** | Frequent non-zero rewards | Faster learning, more guidance | More bias, less emergent behavior |
| **Sparse** | Rare rewards (e.g., only at goal) | Novel solutions possible | Much slower learning |

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Reward Function Forms]:**
- **R(s, a, s')** - Most explicit (depends on full transition)
- **R(s, a)** - Marginalized over next states
- **R(s)** - Depends only on state reached

### Complex Maze Example
- **Barrier states:** Give negative reward (-10) for entering âœ“ Correct
- **Goal state G:** Positive reward (+100) âœ“ Correct
- **Objective:** Agent starts in state B and transitions to G most efficiently âœ“ Correct

ğŸ“š **[ADDITIONAL FROM TEXTBOOK]:**
Even negative values are called "rewards" in RL terminology. Think of them as costs or penalties. The agent's objective is to maximize **cumulative reward** (the return), which means avoiding penalties and reaching positive reward states efficiently.

---

## Episodes and Trajectories

### Episodic Environment
- Agent is "loose" in the maze âœ“ Correct
- Episode: Complete sequence from start to terminal state âœ“ Correct
- Could go on forever if agent doesn't reach goal âœ“ Correct

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Task Types]:**
| Task Type | Description | Example |
|-----------|-------------|---------|
| **Episodic** | Has terminal states, finite | Games, mazes |
| **Continuing** | No natural ending, infinite | Robot locomotion |
| **Indefinite Horizon** | Plans for infinite but may terminate | Most common in RL |

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Terminal State Convention]:**
Terminal states must have:
- All actions transition to themselves with probability 1
- All transitions from terminal state give reward 0

This convention ensures algorithms converge properly (avoids infinite sums).

### Trajectory
Sequence of agent's activities:
```
xâ‚ â†“ 0    xâ‚„ â†‘ 0    xâ‚ .....
Sâ‚€, Aâ‚€, Râ‚, Sâ‚, Aâ‚, Râ‚, Sâ‚‚ ......
```
State â†’ Action â†’ Reward â†’ State â†’ Action â†’ Reward â†’ ... âœ“ Correct

*Note: Time stamps need not be equally spaced* âœ“ Correct

ğŸ“š **[ADDITIONAL FROM TEXTBOOK]:**
A **trajectory** (also called a **rollout**) is the complete sequence:
```
Ï„ = (Sâ‚€, Aâ‚€, Râ‚, Sâ‚, Aâ‚, Râ‚‚, Sâ‚‚, ..., Sâ‚œ)
```
The **return** is the sum of all rewards in a trajectory (possibly discounted).

---

## Policy

### Definition
A **policy** describes what action should be performed by the agent in which state. âœ“ Correct

- A "good policy" is to be learned by an agent âœ“ Correct
- Agent learns an **"optimal" policy** by training âœ“ Correct
- At the end state, it needs to compute what it did and learn âœ“ Correct

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Policy Types]:**
| Policy Type | Notation | Description |
|-------------|----------|-------------|
| **Deterministic** | Ï€(s) = a | Always same action in state s |
| **Stochastic** | Ï€(a\|s) = P(Aâ‚œ=a \| Sâ‚œ=s) | Probability distribution over actions |

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Optimal Policy]:**
The **optimal policy Ï€*** is the policy that maximizes expected return from every state. There may be multiple optimal policies, but they all achieve the same optimal value function.

### Generic Episode
```
Sâ‚€ â†’ ... â†’ Sâ‚œ --Aâ‚œ--> Sâ‚œâ‚Šâ‚ --Râ‚œâ‚Šâ‚--> ... â†’ Sâ‚œ = goal
```
âœ“ Correct

---

## Value Functions

Reinforcement learning usually involves learning **two values**: âœ“ Correct

### 1. State Value Function V(s)
- Value of being in state S âœ“ Correct
- V(s) for all S âˆˆ States âœ“ Correct

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Formal Definition]:**
V^Ï€(s) = Expected return starting from state s, following policy Ï€
```
V^Ï€(s) = E_Ï€[Gâ‚œ | Sâ‚œ = s] = E_Ï€[Î£ Î³áµ Râ‚œâ‚Šâ‚–â‚Šâ‚ | Sâ‚œ = s]
```

### 2. Action Value Function Q(S, A)
- Value of agent performing action A in state S âœ“ Correct
- Q(S, A) for all state-action pairs âœ“ Correct

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Formal Definition]:**
Q^Ï€(s, a) = Expected return starting from state s, taking action a, then following policy Ï€
```
Q^Ï€(s, a) = E_Ï€[Gâ‚œ | Sâ‚œ = s, Aâ‚œ = a]
```

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Relationship Between V and Q]:**
```
V^Ï€(s) = Î£â‚ Ï€(a|s) Ã— Q^Ï€(s, a)
```
The state value is the weighted average of action values under the policy.

*Some algorithms try to learn both V(s) and Q(S, A)* âœ“ Correct

---

## RL Algorithm Types

### Type 1: Learning V(s) - State Values

**Example maze with learned V(s) values:**
```
xâ‚=90  â†’  xâ‚‚=100  â†’  xâ‚ƒ=G (Goal)
  â†‘         â†‘          â†‘
xâ‚„=81  â†’  xâ‚…=90   â†’  xâ‚†=100
```
âœ“ Correct

**Deriving policy from V(s):**
- From state S, go to neighbor state S' where **R' + V(S')** is best âœ“ Correct
- Break ties randomly âœ“ Correct

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Why R + V(s')]:**
This formula comes from the **Bellman equation**:
```
V(s) = max_a [R(s,a,s') + Î³ V(s')]
```
When Î³ â‰ˆ 1, the optimal action is the one that maximizes immediate reward R plus the value of the next state V(s').

### Type 2: Learning Q(S, A) - Action Values

**Example maze with Q(S, A) values:**
```
        xâ‚        xâ‚‚        xâ‚ƒ=Goal
      â†’:90      â†’:100      â†’:âˆ
      â†“:0       â†“:81       (Goal)

        xâ‚„        xâ‚…        xâ‚†
      â†‘:71      â†‘:81      â†‘:100
      â†’:72      â†’:90      â†’:81
      â†“:81
```
âœ“ Correct

*Values of actions, not immediate rewards* âœ“ Correct - Important distinction!

**Deriving policy from Q(S, A):**
- In state S, choose action A that maximizes Q(S, A) âœ“ Correct

ğŸ“š **[ADDITIONAL FROM TEXTBOOK - Q-Learning Preview]:**
This is the foundation of **Q-learning** and **DQN** (which you'll implement for your project!):
```
Ï€*(s) = argmax_a Q*(s, a)
```
The optimal policy simply picks the action with highest Q-value in each state.

---

## Actor-Critic Methods

**Actor-Critic methods** compute both:
- V(s) - State values (Critic) âœ“ Correct
- Q(S, A) - Action values (Actor)

âš ï¸ **[CLARIFICATION NEEDED]:**
The terminology in your notes is slightly imprecise. More accurately:

| Component | What It Learns | Role |
|-----------|----------------|------|
| **Critic** | V(s) or Q(s,a) | Evaluates how good states/actions are |
| **Actor** | Ï€(a\|s) - the policy | Decides which actions to take |

The **Actor** learns the *policy* (mapping states â†’ actions), not Q values directly. The **Critic** provides feedback to help the Actor improve. They work together:
1. Actor proposes an action
2. Critic evaluates "how good was that?"
3. Actor adjusts based on Critic's feedback

ğŸ“š **[ADDITIONAL FROM TEXTBOOK]:**
Actor-Critic methods combine the benefits of:
- **Value-based methods** (like Q-learning): Low variance, but high bias
- **Policy-based methods** (like REINFORCE): Low bias, but high variance

This is why Actor-Critic methods (like A3C, SAC, PPO) are popular in modern deep RL.

---

## Key Takeaways

1. **Markov Property:** Future depends only on present state, not history âœ“ Correct
2. **MDP Components:** States, Actions, Transitions, Rewards âœ“ Correct
3. **Policy:** Mapping from states to actions âœ“ Correct
4. **Value Functions:** V(s) for states, Q(S,A) for state-action pairs âœ“ Correct
5. **Goal:** Learn optimal policy through experience âœ“ Correct

ğŸ“š **[ADDITIONAL KEY TAKEAWAYS FROM TEXTBOOK]:**

6. **Discount Factor (Î³):** Makes future rewards less valuable than immediate rewards
   - Î³ = 0: Only care about immediate reward (greedy)
   - Î³ = 0.99: Care about long-term rewards (common choice)
   - Î³ = 1: No discounting (can cause infinite sums)

7. **Stochastic vs Deterministic Transitions:**
   - Deterministic: T(s,a,s') = 1 for exactly one s'
   - Stochastic: Multiple possible next states (like slippery frozen lake)

8. **The Stationarity Assumption:** Transition and reward functions don't change during training

9. **State Space Can Be:**
   - Discrete (finite states) - like grid worlds
   - Continuous (infinite states) - like robot joint angles

10. **Partially Observable MDPs (POMDPs):** When agent can't see full state
    - Observations â‰  States
    - More realistic but harder to solve

---

## Summary: Your Notes vs. Textbook

| Topic | Your Notes | Textbook Alignment |
|-------|------------|-------------------|
| Markov Property | âœ“ Correct | Perfect match |
| MDP Definition | Partial (S, A only) | Add T, R, Î³ |
| Rewards | âœ“ Correct | Add dense/sparse distinction |
| Episodes | âœ“ Correct | Add task types |
| Policy | âœ“ Correct | Add deterministic/stochastic |
| V(s) and Q(s,a) | âœ“ Correct | Add formal definitions |
| Policy from V(s) | âœ“ Correct | Add Bellman connection |
| Policy from Q(s,a) | âœ“ Correct | Foundation of Q-learning |
| Actor-Critic | âš ï¸ Slight imprecision | Critic=value, Actor=policy |

**Overall Assessment:** Your lecture notes are technically sound and capture the key concepts correctly. The main areas to supplement from the textbook are the complete MDP definition (adding T, R, Î³) and the Actor-Critic clarification.

---
*Enhanced from lecture notes with Chapter 2 cross-reference*
*Prepared: January 28, 2026*
