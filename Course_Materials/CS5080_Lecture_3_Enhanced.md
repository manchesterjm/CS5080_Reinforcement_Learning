# CS 5080 Lecture 3 - Markov Decision Processes (Enhanced)
**Date:** Tuesday, January 27, 2026, 4:45 PM
**Source:** Chapter 2 of Grokking Deep Reinforcement Learning (Morales)

**Legend:**
- Regular text = Your lecture notes (verified correct)
- ‚ö†Ô∏è = Correction or clarification needed
- üìö **[ADDITIONAL FROM TEXTBOOK]** = Supplementary information from Chapter 2

---

## Markov Chains

### Weather Example
- **States:** S = {cold, rain, sun}
- **Transition Table T:**

| From/To | Cold | Rain | Sun |
|---------|------|------|-----|
| Cold    | 0.2  | 0.5  | 0.3 |
| Rain    | 0.5  | 0.3  | 0.2 |
| Sun     | 0.5  | 0.1  | 0.4 |

*Note: Each row sums to 1* ‚úì Correct

üìö **[ADDITIONAL FROM TEXTBOOK]:** This table is called a **transition matrix** or **stochastic matrix**. Each entry T[i,j] represents P(next_state = j | current_state = i). The requirement that rows sum to 1 ensures it's a valid probability distribution.

### Markov Property (Simplifying Assumption)
- **The current state encapsulates all effects of history** ‚úì Correct
- Next transition depends *only* on the current state ‚úì Correct
- History of the chain: C, R, R, R, S, S, C, S, ...
  - "What will it be tomorrow?" depends on current state only ‚úì Correct

üìö **[ADDITIONAL FROM TEXTBOOK - Formal Definition]:**
> P(S‚Çú‚Çä‚ÇÅ | S‚Çú, A‚Çú, S‚Çú‚Çã‚ÇÅ, A‚Çú‚Çã‚ÇÅ, ..., S‚ÇÄ, A‚ÇÄ) = P(S‚Çú‚Çä‚ÇÅ | S‚Çú, A‚Çú)

The probability of the next state given the current state and action is **independent of all previous states and actions**. This is the **memoryless property**.

üìö **[ADDITIONAL FROM TEXTBOOK - Why This Matters]:**
Because RL agents are designed assuming Markov property, you must feed agents all necessary variables to make the property hold. For example:
- **Spacecraft landing:** Need position AND velocity (not just position)
- **Grid worlds:** Cell ID alone is sufficient
- The more variables you add ‚Üí longer training but more accurate
- Fewer variables ‚Üí faster but may not learn well

### Stock Market Example
States: Bear Market, Bull Market, Stagnant Market
- Bear ‚Üí Bull: 0.3
- Bear ‚Üí Bear: 0.5
- Bear ‚Üí Stagnant: 0.2
- (with self-loops and other transitions) ‚úì Correct

---

## Markov Decision Process (MDP)

### Definition (Finite MDP)
- **S** = set of states = {x‚ÇÅ, x‚ÇÇ, ..., x‚Çô} ‚úì Correct
- **A** = set of actions = {a‚ÇÅ, a‚ÇÇ, ..., a‚Çò} ‚úì Correct
- **Agent** interacts with environment ‚úì Correct
- At time t: S‚Çú ‚àà States, A‚Çú ‚àà Actions ‚úì Correct

üìö **[ADDITIONAL FROM TEXTBOOK - Complete MDP Definition]:**
An MDP is formally defined by the tuple: **(S, A, T, R, Œ≥, H, S·µ¢)**

| Component                   | Symbol         | Description                          |
|-----------------------------|----------------|--------------------------------------|
| State Space                 | S              | Set of all possible states           |
| Action Space                | A(s)           | Set of actions available in state s  |
| **Transition Function**     | **T(s,a,s')**  | **Probability P(s' \| s, a)**        |
| **Reward Function**         | **R(s,a,s')**  | **Scalar reward for transition**     |
| **Discount Factor**         | **Œ≥**          | **Value in [0,1], typically 0.99**   |
| Horizon                     | H              | Finite or infinite time steps        |
| Initial State Distribution  | S·µ¢             | Where episodes start                 |

*Your notes covered S and A. The transition function T, reward function R, and discount factor Œ≥ are critical components covered later in the chapter.*

### Agent-Environment Interaction
- Agent can perform a set of actions A = {a‚ÇÅ, a‚ÇÇ, ..., a‚Çò} ‚úì Correct
- Whenever an agent performs an action, it gets an **immediate reward R‚Çú‚Çä‚ÇÅ** ‚úì Correct
- It transitions to state **S‚Çú‚Çä‚ÇÅ** ‚úì Correct

üìö **[ADDITIONAL FROM TEXTBOOK - The RL Cycle]:**
```
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                                      ‚îÇ
    ‚ñº                                      ‚îÇ
  Agent ‚îÄ‚îÄAction A‚Çú‚îÄ‚îÄ‚ñ∫ Environment         ‚îÇ
    ‚ñ≤                      ‚îÇ               ‚îÇ
    ‚îÇ                      ‚îÇ               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ S‚Çú‚Çä‚ÇÅ, R‚Çú‚Çä‚ÇÅ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
                                           ‚îÇ
    (Repeat for each time step t) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

üìö **[ADDITIONAL FROM TEXTBOOK - Experience Tuple]:**
Each interaction produces an **experience tuple**: (S‚Çú, A‚Çú, R‚Çú‚Çä‚ÇÅ, S‚Çú‚Çä‚ÇÅ)
- This is the fundamental unit of data for learning

### "Maze" as Markov Chain vs MDP
**Markov Chain:** States only, no agent control ‚úì Correct
**MDP:** Agent chooses actions to influence transitions ‚úì Correct

**Maze MDP:**
- S = {x‚ÇÅ, x‚ÇÇ, ..., x‚ÇÜ} ‚úì Correct
- A = {‚Üë, ‚Üì, ‚Üí, ‚Üê} ‚úì Correct
- The agent performs action A‚Çú in state S‚Çú to make transitions happen ‚úì Correct

üìö **[ADDITIONAL FROM TEXTBOOK]:**
The key difference: In a Markov Chain, transitions happen automatically according to probabilities. In an MDP, the **agent chooses** which action to take, and then transitions happen according to T(s,a,s').

### Rewards
- **Sparse rewards:** No immediate reward, only on reaching goal state ‚úì Correct
- **Example - Chess:** Taking pieces vs. winning ‚úì Correct
  - Rewards for taking pieces vs. only reward for winning
  - Different reward structures lead to different behaviors ‚úì Correct

üìö **[ADDITIONAL FROM TEXTBOOK - Dense vs Sparse Rewards]:**
| Reward Type | Description                       | Pros                           | Cons                            |
|-------------|-----------------------------------|--------------------------------|---------------------------------|
| **Dense**   | Frequent non-zero rewards         | Faster learning, more guidance | More bias, less emergent behavior |
| **Sparse**  | Rare rewards (e.g., only at goal) | Novel solutions possible       | Much slower learning            |

üìö **[ADDITIONAL FROM TEXTBOOK - Reward Function Forms]:**
- **R(s, a, s')** - Most explicit (depends on full transition)
- **R(s, a)** - Marginalized over next states
- **R(s)** - Depends only on state reached

### Complex Maze Example
- **Barrier states:** Give negative reward (-10) for entering ‚úì Correct
- **Goal state G:** Positive reward (+100) ‚úì Correct
- **Objective:** Agent starts in state B and transitions to G most efficiently ‚úì Correct

üìö **[ADDITIONAL FROM TEXTBOOK]:**
Even negative values are called "rewards" in RL terminology. Think of them as costs or penalties. The agent's objective is to maximize **cumulative reward** (the return), which means avoiding penalties and reaching positive reward states efficiently.

---

## Episodes and Trajectories

### Episodic Environment
- Agent is "loose" in the maze ‚úì Correct
- Episode: Complete sequence from start to terminal state ‚úì Correct
- Could go on forever if agent doesn't reach goal ‚úì Correct

üìö **[ADDITIONAL FROM TEXTBOOK - Task Types]:**
| Task Type              | Description                        | Example            |
|------------------------|------------------------------------|--------------------|
| **Episodic**           | Has terminal states, finite        | Games, mazes       |
| **Continuing**         | No natural ending, infinite        | Robot locomotion   |
| **Indefinite Horizon** | Plans for infinite but may terminate | Most common in RL |

üìö **[ADDITIONAL FROM TEXTBOOK - Terminal State Convention]:**
Terminal states must have:
- All actions transition to themselves with probability 1
- All transitions from terminal state give reward 0

This convention ensures algorithms converge properly (avoids infinite sums).

### Trajectory
Sequence of agent's activities:
```
x‚ÇÅ ‚Üì 0    x‚ÇÑ ‚Üë 0    x‚ÇÅ .....
S‚ÇÄ, A‚ÇÄ, R‚ÇÅ, S‚ÇÅ, A‚ÇÅ, R‚ÇÅ, S‚ÇÇ ......
```
State ‚Üí Action ‚Üí Reward ‚Üí State ‚Üí Action ‚Üí Reward ‚Üí ... ‚úì Correct

*Note: Time stamps need not be equally spaced* ‚úì Correct

üìö **[ADDITIONAL FROM TEXTBOOK]:**
A **trajectory** (also called a **rollout**) is the complete sequence:
```
œÑ = (S‚ÇÄ, A‚ÇÄ, R‚ÇÅ, S‚ÇÅ, A‚ÇÅ, R‚ÇÇ, S‚ÇÇ, ..., S‚Çú)
```
The **return** is the sum of all rewards in a trajectory (possibly discounted).

---

## Policy

### Definition
A **policy** describes what action should be performed by the agent in which state. ‚úì Correct

- A "good policy" is to be learned by an agent ‚úì Correct
- Agent learns an **"optimal" policy** by training ‚úì Correct
- At the end state, it needs to compute what it did and learn ‚úì Correct

üìö **[ADDITIONAL FROM TEXTBOOK - Policy Types]:**
| Policy Type       | Notation                    | Description                            |
|-------------------|-----------------------------|----------------------------------------|
| **Deterministic** | œÄ(s) = a                    | Always same action in state s          |
| **Stochastic**    | œÄ(a\|s) = P(A‚Çú=a \| S‚Çú=s)   | Probability distribution over actions  |

üìö **[ADDITIONAL FROM TEXTBOOK - Optimal Policy]:**
The **optimal policy œÄ*** is the policy that maximizes expected return from every state. There may be multiple optimal policies, but they all achieve the same optimal value function.

### Generic Episode
```
S‚ÇÄ ‚Üí ... ‚Üí S‚Çú --A‚Çú--> S‚Çú‚Çä‚ÇÅ --R‚Çú‚Çä‚ÇÅ--> ... ‚Üí S‚Çú = goal
```
‚úì Correct

---

## Value Functions

Reinforcement learning usually involves learning **two values**: ‚úì Correct

### 1. State Value Function V(s)
- Value of being in state S ‚úì Correct
- V(s) for all S ‚àà States ‚úì Correct

üìö **[ADDITIONAL FROM TEXTBOOK - Formal Definition]:**
V^œÄ(s) = Expected return starting from state s, following policy œÄ
```
V^œÄ(s) = E_œÄ[G‚Çú | S‚Çú = s] = E_œÄ[Œ£ Œ≥·µè R‚Çú‚Çä‚Çñ‚Çä‚ÇÅ | S‚Çú = s]
```

### 2. Action Value Function Q(S, A)
- Value of agent performing action A in state S ‚úì Correct
- Q(S, A) for all state-action pairs ‚úì Correct

üìö **[ADDITIONAL FROM TEXTBOOK - Formal Definition]:**
Q^œÄ(s, a) = Expected return starting from state s, taking action a, then following policy œÄ
```
Q^œÄ(s, a) = E_œÄ[G‚Çú | S‚Çú = s, A‚Çú = a]
```

üìö **[ADDITIONAL FROM TEXTBOOK - Relationship Between V and Q]:**
```
V^œÄ(s) = Œ£‚Çê œÄ(a|s) √ó Q^œÄ(s, a)
```
The state value is the weighted average of action values under the policy.

*Some algorithms try to learn both V(s) and Q(S, A)* ‚úì Correct

---

## RL Algorithm Types

### Type 1: Learning V(s) - State Values

**Example maze with learned V(s) values:**
```
x‚ÇÅ=90  ‚Üí  x‚ÇÇ=100  ‚Üí  x‚ÇÉ=G (Goal)
  ‚Üë         ‚Üë          ‚Üë
x‚ÇÑ=81  ‚Üí  x‚ÇÖ=90   ‚Üí  x‚ÇÜ=100
```
‚úì Correct

**Deriving policy from V(s):**
- From state S, go to neighbor state S' where **R' + V(S')** is best ‚úì Correct
- Break ties randomly ‚úì Correct

üìö **[ADDITIONAL FROM TEXTBOOK - Why R + V(s')]:**
This formula comes from the **Bellman equation**:
```
V(s) = max_a [R(s,a,s') + Œ≥ V(s')]
```
When Œ≥ ‚âà 1, the optimal action is the one that maximizes immediate reward R plus the value of the next state V(s').

### Type 2: Learning Q(S, A) - Action Values

**Example maze with Q(S, A) values:**
```
        x‚ÇÅ        x‚ÇÇ        x‚ÇÉ=Goal
      ‚Üí:90      ‚Üí:100      ‚Üí:‚àû
      ‚Üì:0       ‚Üì:81       (Goal)

        x‚ÇÑ        x‚ÇÖ        x‚ÇÜ
      ‚Üë:71      ‚Üë:81      ‚Üë:100
      ‚Üí:72      ‚Üí:90      ‚Üí:81
      ‚Üì:81
```
‚úì Correct

*Values of actions, not immediate rewards* ‚úì Correct - Important distinction!

**Deriving policy from Q(S, A):**
- In state S, choose action A that maximizes Q(S, A) ‚úì Correct

üìö **[ADDITIONAL FROM TEXTBOOK - Q-Learning Preview]:**
This is the foundation of **Q-learning** and **DQN** (which you'll implement for your project!):
```
œÄ*(s) = argmax_a Q*(s, a)
```
The optimal policy simply picks the action with highest Q-value in each state.

---

## Actor-Critic Methods

**Actor-Critic methods** compute both:
- V(s) - State values (Critic) ‚úì Correct
- Q(S, A) - Action values (Actor)

‚ö†Ô∏è **[CLARIFICATION NEEDED]:**
The terminology in your notes is slightly imprecise. More accurately:

| Component  | What It Learns       | Role                                  |
|------------|----------------------|---------------------------------------|
| **Critic** | V(s) or Q(s,a)       | Evaluates how good states/actions are |
| **Actor**  | œÄ(a\|s) - the policy | Decides which actions to take         |

The **Actor** learns the *policy* (mapping states ‚Üí actions), not Q values directly. The **Critic** provides feedback to help the Actor improve. They work together:
1. Actor proposes an action
2. Critic evaluates "how good was that?"
3. Actor adjusts based on Critic's feedback

üìö **[ADDITIONAL FROM TEXTBOOK]:**
Actor-Critic methods combine the benefits of:
- **Value-based methods** (like Q-learning): Low variance, but high bias
- **Policy-based methods** (like REINFORCE): Low bias, but high variance

This is why Actor-Critic methods (like A3C, SAC, PPO) are popular in modern deep RL.

---

## Key Takeaways

1. **Markov Property:** Future depends only on present state, not history ‚úì Correct
2. **MDP Components:** States, Actions, Transitions, Rewards ‚úì Correct
3. **Policy:** Mapping from states to actions ‚úì Correct
4. **Value Functions:** V(s) for states, Q(S,A) for state-action pairs ‚úì Correct
5. **Goal:** Learn optimal policy through experience ‚úì Correct

üìö **[ADDITIONAL KEY TAKEAWAYS FROM TEXTBOOK]:**

6. **Discount Factor (Œ≥):** Makes future rewards less valuable than immediate rewards
   - Œ≥ = 0: Only care about immediate reward (greedy)
   - Œ≥ = 0.99: Care about long-term rewards (common choice)
   - Œ≥ = 1: No discounting (can cause infinite sums)

7. **Stochastic vs Deterministic Transitions:**
   - Deterministic: T(s,a,s') = 1 for exactly one s'
   - Stochastic: Multiple possible next states (like slippery frozen lake)

8. **The Stationarity Assumption:** Transition and reward functions don't change during training

9. **State Space Can Be:**
   - Discrete (finite states) - like grid worlds
   - Continuous (infinite states) - like robot joint angles

10. **Partially Observable MDPs (POMDPs):** When agent can't see full state
    - Observations ‚â† States
    - More realistic but harder to solve

---

## Summary: Your Notes vs. Textbook

| Topic              | Your Notes            | Textbook Alignment            |
|--------------------|-----------------------|-------------------------------|
| Markov Property    | ‚úì Correct             | Perfect match                 |
| MDP Definition     | Partial (S, A only)   | Add T, R, Œ≥                   |
| Rewards            | ‚úì Correct             | Add dense/sparse distinction  |
| Episodes           | ‚úì Correct             | Add task types                |
| Policy             | ‚úì Correct             | Add deterministic/stochastic  |
| V(s) and Q(s,a)    | ‚úì Correct             | Add formal definitions        |
| Policy from V(s)   | ‚úì Correct             | Add Bellman connection        |
| Policy from Q(s,a) | ‚úì Correct             | Foundation of Q-learning      |
| Actor-Critic       | ‚ö†Ô∏è Slight imprecision | Critic=value, Actor=policy    |

**Overall Assessment:** Your lecture notes are technically sound and capture the key concepts correctly. The main areas to supplement from the textbook are the complete MDP definition (adding T, R, Œ≥) and the Actor-Critic clarification.

---
*Enhanced from lecture notes with Chapter 2 cross-reference*
*Prepared: January 28, 2026*
